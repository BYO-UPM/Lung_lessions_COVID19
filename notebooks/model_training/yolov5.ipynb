{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-11-04T19:51:55.400279Z","iopub.status.busy":"2022-11-04T19:51:55.399720Z","iopub.status.idle":"2022-11-04T19:52:07.544563Z","shell.execute_reply":"2022-11-04T19:52:07.543156Z","shell.execute_reply.started":"2022-11-04T19:51:55.400134Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'yolov5'...\n","remote: Enumerating objects: 10485, done.\u001b[K\n","remote: Counting objects: 100% (4/4), done.\u001b[K\n","remote: Compressing objects: 100% (4/4), done.\u001b[K\n","remote: Total 10485 (delta 0), reused 1 (delta 0), pack-reused 10481\u001b[K\n","Receiving objects: 100% (10485/10485), 12.29 MiB | 34.56 MiB/s, done.\n","Resolving deltas: 100% (7054/7054), done.\n","/kaggle/working/yolov5\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"]}],"source":["!git clone https://github.com/alvaromoure/yolov5.git\n","%cd yolov5\n","%pip install -qr requirements.txt  # install dependencies"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T19:52:07.547687Z","iopub.status.busy":"2022-11-04T19:52:07.547362Z","iopub.status.idle":"2022-11-04T19:52:07.557643Z","shell.execute_reply":"2022-11-04T19:52:07.556477Z","shell.execute_reply.started":"2022-11-04T19:52:07.547661Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/working\n"]}],"source":["%cd .."]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-08-22T10:17:28.168764Z","iopub.status.busy":"2022-08-22T10:17:28.167345Z","iopub.status.idle":"2022-08-22T10:17:41.458755Z","shell.execute_reply":"2022-08-22T10:17:41.457495Z","shell.execute_reply.started":"2022-08-22T10:17:28.168727Z"}},"source":["!pip install -q --upgrade wandb"]},{"cell_type":"markdown","metadata":{},"source":["# Imports"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T19:52:07.559635Z","iopub.status.busy":"2022-11-04T19:52:07.559327Z","iopub.status.idle":"2022-11-04T19:52:10.135301Z","shell.execute_reply":"2022-11-04T19:52:10.134201Z","shell.execute_reply.started":"2022-11-04T19:52:07.559606Z"},"trusted":true},"outputs":[],"source":["import torch\n","import wandb\n","import os\n","import gc\n","import cv2\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import shutil\n","import matplotlib.pyplot as plt\n","import json"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T19:52:11.161772Z","iopub.status.busy":"2022-11-04T19:52:11.161446Z","iopub.status.idle":"2022-11-04T19:52:11.167371Z","shell.execute_reply":"2022-11-04T19:52:11.166018Z","shell.execute_reply.started":"2022-11-04T19:52:11.161744Z"},"trusted":true},"outputs":[],"source":["FOLD = 0\n","EPOCHS = 300"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T19:52:11.169929Z","iopub.status.busy":"2022-11-04T19:52:11.169521Z","iopub.status.idle":"2022-11-04T19:52:11.181595Z","shell.execute_reply":"2022-11-04T19:52:11.180446Z","shell.execute_reply.started":"2022-11-04T19:52:11.169871Z"},"trusted":true},"outputs":[],"source":["PROJECT_NAME = 'Final-Covid19-Detection'\n","NOTEBOOK_NAME = ''\n","EXP_NAME = F'yoloV5x_300e_fold{FOLD}'"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T19:52:19.462812Z","iopub.status.busy":"2022-11-04T19:52:19.462379Z","iopub.status.idle":"2022-11-04T19:52:19.466059Z","shell.execute_reply":"2022-11-04T19:52:19.465385Z","shell.execute_reply.started":"2022-11-04T19:52:19.462783Z"},"trusted":true},"outputs":[],"source":["train_ann_file = f'/kaggle/working/coco_train_512x512_fold_{FOLD}.json'\n","val_ann_file = f'/kaggle/working/coco_val_512x512_fold_{FOLD}.json'"]},{"cell_type":"markdown","metadata":{},"source":["## Creating YoloV5 folder structure using annotations files"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T19:52:19.467387Z","iopub.status.busy":"2022-11-04T19:52:19.467020Z","iopub.status.idle":"2022-11-04T19:52:19.480834Z","shell.execute_reply":"2022-11-04T19:52:19.479181Z","shell.execute_reply.started":"2022-11-04T19:52:19.467363Z"},"trusted":true},"outputs":[],"source":["os.makedirs('/kaggle/working/dataset')\n","os.makedirs('/kaggle/working/dataset/images')\n","os.makedirs('/kaggle/working/dataset/images/train')\n","os.makedirs('/kaggle/working/dataset/images/val')\n","os.makedirs('/kaggle/working/dataset/labels')\n","os.makedirs('/kaggle/working/dataset/labels/train')\n","os.makedirs('/kaggle/working/dataset/labels/val')\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T19:52:19.483074Z","iopub.status.busy":"2022-11-04T19:52:19.482512Z","iopub.status.idle":"2022-11-04T19:52:19.666433Z","shell.execute_reply":"2022-11-04T19:52:19.665271Z","shell.execute_reply.started":"2022-11-04T19:52:19.483044Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3435/3435 [00:00<00:00, 1678992.45it/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 859/859 [00:00<00:00, 1424073.97it/s]\n"]}],"source":["with open(train_ann_file,'rb') as file:\n","    train_dict = json.load(file)\n","train_image_paths = []\n","for item in tqdm(train_dict['images']):\n","    train_image_paths.append(item['file_name'])\n","                            \n","                             \n","with open(val_ann_file,'rb') as file:\n","    val_dict = json.load(file)\n","val_image_paths = []\n","for item in tqdm(val_dict['images']):\n","    val_image_paths.append(item['file_name'])\n","    \n","    \n","                             "]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T19:52:19.669317Z","iopub.status.busy":"2022-11-04T19:52:19.668915Z","iopub.status.idle":"2022-11-04T19:52:38.814832Z","shell.execute_reply":"2022-11-04T19:52:38.813946Z","shell.execute_reply.started":"2022-11-04T19:52:19.669280Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3435/3435 [00:15<00:00, 221.95it/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 859/859 [00:03<00:00, 234.93it/s]\n"]}],"source":["for image in tqdm(train_image_paths):\n","    image_id = image.split('_')[0]\n","    shutil.copyfile(os.path.join('/kaggle/input/siim-covid19-images-metadata-256-512-768/images_metadata_256_512_768/train_512x512',\n","                                 image),f'/kaggle/working/dataset/images/train/{image_id}.png')\n","for image in tqdm(val_image_paths):\n","    image_id = image.split('_')[0]\n","    shutil.copyfile(os.path.join('/kaggle/input/siim-covid19-images-metadata-256-512-768/images_metadata_256_512_768/train_512x512',\n","                                 image),f'/kaggle/working/dataset/images/val/{image_id}.png')\n","    "]},{"cell_type":"markdown","metadata":{},"source":["### Creating the data YAML file"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T19:52:38.816726Z","iopub.status.busy":"2022-11-04T19:52:38.816210Z","iopub.status.idle":"2022-11-04T19:52:38.823609Z","shell.execute_reply":"2022-11-04T19:52:38.822435Z","shell.execute_reply.started":"2022-11-04T19:52:38.816694Z"},"trusted":true},"outputs":[],"source":["import yaml\n","data_yaml = dict(\n","    path = '/kaggle/working/dataset',\n","    train = 'images/train/',\n","    val = 'images/val/',\n","    nc=1,\n","    names = ['Covid_Opacity',]\n",")\n","\n","with open('/kaggle/working/yolov5/data/data.yaml','w') as file:\n","    yaml.dump(data_yaml,file,default_flow_style=True)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T19:52:38.825140Z","iopub.status.busy":"2022-11-04T19:52:38.824743Z","iopub.status.idle":"2022-11-04T19:52:38.841210Z","shell.execute_reply":"2022-11-04T19:52:38.839948Z","shell.execute_reply.started":"2022-11-04T19:52:38.825107Z"},"trusted":true},"outputs":[],"source":["IMG_SIZE = 512\n","\n","def get_bbox(row):\n","    bboxes = []\n","    bbox = []\n","    for i, l in enumerate(row.label.split(' ')):\n","        if (i % 6 == 0) | (i % 6 == 1):\n","            continue\n","        bbox.append(float(l))\n","        if i % 6 == 5:\n","            bboxes.append(bbox)\n","            bbox = []  \n","            \n","    return bboxes\n","\n","# Scale the bounding boxes according to the size of the resized image. \n","def scale_bbox(row, bboxes):\n","    # Get scaling factor\n","    scale_x = IMG_SIZE/row.dim1\n","    scale_y = IMG_SIZE/row.dim0\n","    \n","    scaled_bboxes = []\n","    for bbox in bboxes:\n","        x = int(np.round(bbox[0]*scale_x, 4))\n","        y = int(np.round(bbox[1]*scale_y, 4))\n","        x1 = int(np.round(bbox[2]*(scale_x), 4))\n","        y1= int(np.round(bbox[3]*scale_y, 4))\n","\n","        scaled_bboxes.append([x, y, x1, y1]) # xmin, ymin, xmax, ymax\n","        \n","    return scaled_bboxes\n","\n","# Convert the bounding boxes in YOLO format.\n","def get_yolo_format_bbox(img_w, img_h, bboxes):\n","    yolo_boxes = []\n","    for bbox in bboxes:\n","        w = bbox[2] - bbox[0] # xmax - xmin\n","        h = bbox[3] - bbox[1] # ymax - ymin\n","        xc = bbox[0] + int(np.round(w/2)) # xmin + width/2\n","        yc = bbox[1] + int(np.round(h/2)) # ymin + height/2\n","        \n","        yolo_boxes.append([xc/img_w, yc/img_h, w/img_w, h/img_h]) # x_center y_center width height\n","    \n","    return yolo_boxes"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T19:52:38.842637Z","iopub.status.busy":"2022-11-04T19:52:38.842347Z","iopub.status.idle":"2022-11-04T19:52:39.197057Z","shell.execute_reply":"2022-11-04T19:52:39.195726Z","shell.execute_reply.started":"2022-11-04T19:52:38.842610Z"},"trusted":true},"outputs":[],"source":["df = pd.read_csv('/kaggle/input/siim-covid19-detection/train_image_level.csv')\n","df['id'] = df.apply(lambda row: row.id.split('_')[0], axis=1)\n","df['image_level'] = df.apply(lambda row: row.label.split(' ')[0], axis=1)\n","meta_df = pd.read_csv(f'/kaggle/input/siim-covid-19-256-512-768-1024-1280/train_meta_{IMG_SIZE}x{IMG_SIZE}.csv')\n","meta_df['id'] = meta_df.apply(lambda row: row.id.split('_')[0], axis=1)\n","meta_df.drop('folder_id',inplace=True,axis=1)\n","meta_df.drop('study_id',inplace=True,axis=1)\n","meta_df['dim0'] = meta_df['height']\n","meta_df['dim1'] = meta_df['width']\n","meta_df.drop('width',inplace=True,axis=1)\n","meta_df.drop('height',inplace=True,axis=1)\n","df = df.merge(meta_df, on='id',how=\"left\")"]},{"cell_type":"markdown","metadata":{},"source":["## Preprocesing the images off-line\n","In this case we apply the preprocesing (histogram equalization and CLAHE in an off-line fashio due to its easier to implement for YOLO model)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T19:52:39.198948Z","iopub.status.busy":"2022-11-04T19:52:39.198612Z","iopub.status.idle":"2022-11-04T19:53:11.091462Z","shell.execute_reply":"2022-11-04T19:53:11.090257Z","shell.execute_reply.started":"2022-11-04T19:52:39.198919Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3435/3435 [00:31<00:00, 107.76it/s]\n"]}],"source":["BASE_PATH_TRAIN = '/kaggle/working/dataset/images/train'\n","BASE_PATH_VAL = '/kaggle/working/dataset/images/val'\n","\n","clahe = cv2.createCLAHE(clipLimit=2.0)\n","for image in tqdm(os.listdir(BASE_PATH_TRAIN)):\n","    full_image_path = os.path.join(BASE_PATH_TRAIN,image)\n","    img = cv2.imread(full_image_path,cv2.IMREAD_UNCHANGED)\n","    img_eq = cv2.equalizeHist(img)\n","    img_clahe = clahe.apply(img_eq)\n","    cv2.imwrite(full_image_path,img_clahe)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T19:53:11.093265Z","iopub.status.busy":"2022-11-04T19:53:11.092935Z","iopub.status.idle":"2022-11-04T19:53:19.061347Z","shell.execute_reply":"2022-11-04T19:53:19.060434Z","shell.execute_reply.started":"2022-11-04T19:53:11.093237Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 859/859 [00:07<00:00, 107.96it/s]\n"]}],"source":["for image in tqdm(os.listdir(BASE_PATH_VAL)):\n","    full_image_path = os.path.join(BASE_PATH_VAL,image)\n","    img = cv2.imread(full_image_path,cv2.IMREAD_UNCHANGED)\n","    img_eq = cv2.equalizeHist(img)\n","    img_clahe = clahe.apply(img_eq)\n","    cv2.imwrite(full_image_path,img_clahe)"]},{"cell_type":"markdown","metadata":{},"source":["## Labels Generation"]},{"cell_type":"markdown","metadata":{},"source":["### Training Labels"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T19:53:19.070573Z","iopub.status.busy":"2022-11-04T19:53:19.070283Z","iopub.status.idle":"2022-11-04T19:53:22.739627Z","shell.execute_reply":"2022-11-04T19:53:22.738407Z","shell.execute_reply.started":"2022-11-04T19:53:19.070546Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["3435it [00:03, 940.08it/s]\n"]}],"source":["for i,image in tqdm(enumerate(train_image_paths)):\n","    image_id = image.split('_')[0]\n","    row = df[df['id']==image_id].iloc[0]\n","    label = row.image_level\n","    if label=='opacity':\n","        bboxes = get_bbox(row)\n","        scale_bboxes = scale_bbox(row,bboxes)\n","        yolo_bboxes = get_yolo_format_bbox(IMG_SIZE,IMG_SIZE,scale_bboxes)\n","        with (open(f'/kaggle/working/dataset/labels/train/{image_id}.txt','w')) as file:\n","            for bbox in yolo_bboxes:\n","                bbox = [0]+bbox\n","                bbox = [str(i) for i in bbox]\n","                bbox = ' '.join(bbox)\n","                file.write(bbox)\n","                file.write('\\n')\n","                "]},{"cell_type":"markdown","metadata":{},"source":["### Validation Labels"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T19:53:22.741220Z","iopub.status.busy":"2022-11-04T19:53:22.740864Z","iopub.status.idle":"2022-11-04T19:53:23.685739Z","shell.execute_reply":"2022-11-04T19:53:23.684491Z","shell.execute_reply.started":"2022-11-04T19:53:22.741194Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["859it [00:00, 921.04it/s]\n"]}],"source":["for i,image in tqdm(enumerate(val_image_paths)):\n","    image_id = image.split('_')[0]\n","    row = df[df['id']==image_id].iloc[0]\n","    label = row.image_level\n","    if label=='opacity':\n","        bboxes = get_bbox(row)\n","        scale_bboxes = scale_bbox(row,bboxes)\n","        yolo_bboxes = get_yolo_format_bbox(IMG_SIZE,IMG_SIZE,scale_bboxes)\n","        with (open(f'/kaggle/working/dataset/labels/val/{image_id}.txt','w')) as file:\n","            for bbox in yolo_bboxes:\n","                bbox = [0]+bbox\n","                bbox = [str(i) for i in bbox]\n","                bbox = ' '.join(bbox)\n","                file.write(bbox)\n","                file.write('\\n')"]},{"cell_type":"markdown","metadata":{},"source":["## Creating the hyperparameters YAML file"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T19:53:23.687144Z","iopub.status.busy":"2022-11-04T19:53:23.686827Z","iopub.status.idle":"2022-11-04T19:53:23.695964Z","shell.execute_reply":"2022-11-04T19:53:23.695221Z","shell.execute_reply.started":"2022-11-04T19:53:23.687119Z"},"trusted":true},"outputs":[],"source":["hyp_yaml = dict(\n","    lr0=0.002,\n","    lrf=0.02,\n","    momentum=0.9,\n","    weight_decay=0.0005,\n","    warmup_epochs=3.0,\n","    warmup_momentum=0.8,\n","    warmup_bias_lr=0.1,\n","    box=0.05,\n","    cls=0.5,\n","    cls_pw=1.0,\n","    obj=1.0,\n","    obj_pw=1.0,\n","    iou_t=0.2,\n","    anchor_t=4.0,\n","    fl_gamma=0.0,\n","    hsv_h=0,\n","    hsv_s=0,\n","    hsv_v=0,\n","    degrees=0,\n","    translate=0,\n","    scale=0,\n","    shear=0.0,\n","    perspective=0.0,\n","    flipud=0.0,\n","    fliplr=0,\n","    mosaic=0,\n","    mixup=0.0,\n","    copy_paste=0.0\n",")\n","\n","with open('/kaggle/working/hyp.yaml','w') as file:\n","    yaml.dump(hyp_yaml,file,default_flow_style=True)"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T19:53:23.697011Z","iopub.status.busy":"2022-11-04T19:53:23.696747Z","iopub.status.idle":"2022-11-04T19:53:23.709554Z","shell.execute_reply":"2022-11-04T19:53:23.708390Z","shell.execute_reply.started":"2022-11-04T19:53:23.696986Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/working/yolov5\n"]}],"source":["%cd yolov5"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T19:53:23.711430Z","iopub.status.busy":"2022-11-04T19:53:23.711054Z","iopub.status.idle":"2022-11-04T19:53:23.719134Z","shell.execute_reply":"2022-11-04T19:53:23.717769Z","shell.execute_reply.started":"2022-11-04T19:53:23.711400Z"},"trusted":true},"outputs":[],"source":["name = f'{EXP_NAME}_{dt_string}'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-04T19:53:23.721598Z","iopub.status.busy":"2022-11-04T19:53:23.721225Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malvaromoureupm\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5x.pt, cfg=, data=data.yaml, hyp=/kaggle/working/hyp.yaml, epochs=300, batch_size=16, imgsz=512, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=Final-Covid19-Detection, name=yoloV5x_300e_fold0_04/11_20:52, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=50, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n","remote: Enumerating objects: 3987, done.\u001b[K\n","remote: Counting objects: 100% (979/979), done.\u001b[K\n","remote: Compressing objects: 100% (17/17), done.\u001b[K\n","remote: Total 3987 (delta 965), reused 968 (delta 962), pack-reused 3008\u001b[K\n","Receiving objects: 100% (3987/3987), 1.19 MiB | 17.92 MiB/s, done.\n","Resolving deltas: 100% (2927/2927), completed with 179 local objects.\n","From https://github.com/ultralytics/yolov5\n"," * [new branch]      add/git               -> ultralytics/add/git\n"," * [new branch]      add/weights_dir       -> ultralytics/add/weights_dir\n"," * [new branch]      cls/test_reset_params -> ultralytics/cls/test_reset_params\n"," * [new branch]      coco-segments         -> ultralytics/coco-segments\n"," * [new branch]      exp/copy-paste        -> ultralytics/exp/copy-paste\n"," * [new branch]      exp/init_bias_lr      -> ultralytics/exp/init_bias_lr\n"," * [new branch]      exp/scaleFill         -> ultralytics/exp/scaleFill\n"," * [new branch]      exp10                 -> ultralytics/exp10\n"," * [new branch]      exp8                  -> ultralytics/exp8\n"," * [new branch]      exp8-temp-bnexp       -> ultralytics/exp8-temp-bnexp\n"," * [new branch]      exp9                  -> ultralytics/exp9\n"," * [new branch]      fix/rgb_albumentations -> ultralytics/fix/rgb_albumentations\n"," * [new branch]      ghost                 -> ultralytics/ghost\n"," * [new branch]      glenn-jocher-patch-1  -> ultralytics/glenn-jocher-patch-1\n"," * [new branch]      master                -> ultralytics/master\n"," * [new branch]      pip                   -> ultralytics/pip\n"," * [new branch]      study_activations     -> ultralytics/study_activations\n"," * [new branch]      test/alive_progress   -> ultralytics/test/alive_progress\n"," * [new branch]      test/conv_reduction   -> ultralytics/test/conv_reduction\n"," * [new branch]      test/convtranspose    -> ultralytics/test/convtranspose\n"," * [new branch]      test/dw5              -> ultralytics/test/dw5\n"," * [new branch]      test/seeds            -> ultralytics/test/seeds\n"," * [new branch]      ultralytics/HUB       -> ultralytics/ultralytics/HUB\n"," * [new branch]      update/cls-album      -> ultralytics/update/cls-album\n"," * [new branch]      update/loss           -> ultralytics/update/loss\n"," * [new branch]      update/textlogger     -> ultralytics/update/textlogger\n"," * [new branch]      update/threaded       -> ultralytics/update/threaded\n"," * [new tag]         v1.0                  -> v1.0\n"," * [new tag]         v2.0                  -> v2.0\n"," * [new tag]         v3.0                  -> v3.0\n"," * [new tag]         v3.1                  -> v3.1\n"," * [new tag]         v4.0                  -> v4.0\n"," * [new tag]         v5.0                  -> v5.0\n"," * [new tag]         v6.0                  -> v6.0\n"," * [new tag]         v6.1                  -> v6.1\n"," * [new tag]         v6.2                  -> v6.2\n","\u001b[34m\u001b[1mgithub: \u001b[0m‚ö†Ô∏è YOLOv5 is out of date by 117 commits. Use `git pull ultralytics master` or `git clone https://github.com/ultralytics/yolov5` to update.\n","YOLOv5 üöÄ v6.2-110-ge478b2e Python-3.7.12 torch-1.11.0+cpu CPU\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0manchor_t=4.0, box=0.05, cls=0.5, cls_pw=1.0, copy_paste=0.0, degrees=10.0, fl_gamma=0.0, fliplr=0.5, flipud=0.0, hsv_h=0, hsv_s=0, hsv_v=0, iou_t=0.2, lr0=0.002, lrf=0.02, mixup=0.0, momentum=0.9, mosaic=0, obj=1.0, obj_pw=1.0, perspective=0.0, scale=0.5, shear=0.0, translate=0.1, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 üöÄ in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir Final-Covid19-Detection/yoloV5x_300e_fold0_04', view at http://localhost:6006/\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.5 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.20\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/yolov5/wandb/run-20221104_195335-i2ojs7uy\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33myoloV5x_300e_fold0_04/11_20:52\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/alvaromoureupm/Final-Covid19-Detection\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/alvaromoureupm/Final-Covid19-Detection/runs/i2ojs7uy\u001b[0m\n","Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 755k/755k [00:00<00:00, 16.3MB/s]\n","YOLOv5 temporarily requires wandb version 0.12.10 or below. Some features may not work as expected.\n","Downloading https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5x.pt to yolov5x.pt...\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 166M/166M [00:01<00:00, 89.7MB/s]\n","\n","Overriding model.yaml nc=80 with nc=1\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      8800  models.common.Conv                      [3, 80, 6, 2, 2]              \n","  1                -1  1    115520  models.common.Conv                      [80, 160, 3, 2]               \n","  2                -1  4    309120  models.common.C3                        [160, 160, 4]                 \n","  3                -1  1    461440  models.common.Conv                      [160, 320, 3, 2]              \n","  4                -1  8   2259200  models.common.C3                        [320, 320, 8]                 \n","  5                -1  1   1844480  models.common.Conv                      [320, 640, 3, 2]              \n","  6                -1 12  13125120  models.common.C3                        [640, 640, 12]                \n","  7                -1  1   7375360  models.common.Conv                      [640, 1280, 3, 2]             \n","  8                -1  4  19676160  models.common.C3                        [1280, 1280, 4]               \n","  9                -1  1   4099840  models.common.SPPF                      [1280, 1280, 5]               \n"," 10                -1  1    820480  models.common.Conv                      [1280, 640, 1, 1]             \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  4   5332480  models.common.C3                        [1280, 640, 4, False]         \n"," 14                -1  1    205440  models.common.Conv                      [640, 320, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  4   1335040  models.common.C3                        [640, 320, 4, False]          \n"," 18                -1  1    922240  models.common.Conv                      [320, 320, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  4   4922880  models.common.C3                        [640, 640, 4, False]          \n"," 21                -1  1   3687680  models.common.Conv                      [640, 640, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  4  19676160  models.common.C3                        [1280, 1280, 4, False]        \n"," 24      [17, 20, 23]  1     40374  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [320, 640, 1280]]\n","Model summary: 567 layers, 86217814 parameters, 86217814 gradients, 204.6 GFLOPs\n","\n","Transferred 739/745 items from yolov5x.pt\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.002) with parameter groups 123 weight(decay=0.0), 126 weight(decay=0.0005), 126 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mRotate(p=0.25, limit=(-10, 10), interpolation=1, border_mode=0, value=0, mask_value=None), HorizontalFlip(p=0.5), OneOf([\n","  MotionBlur(p=0.2, blur_limit=(3, 7)),\n","  MedianBlur(p=0.1, blur_limit=(3, 3)),\n","  Blur(p=0.1, blur_limit=(3, 3)),\n","], p=0.5), RandomBrightnessContrast(p=0.2, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), brightness_by_max=True)\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/kaggle/working/dataset/labels/train' images and labels...3435 \u001b[0m\n","\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /kaggle/working/dataset/labels/train.cache\n","\u001b[34m\u001b[1mval: \u001b[0mScanning '/kaggle/working/dataset/labels/val' images and labels...859 found\u001b[0m\n","\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /kaggle/working/dataset/labels/val.cache\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.80 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ‚úÖ\n","Plotting labels to Final-Covid19-Detection/yoloV5x_300e_fold0_04/11_20:52/labels.jpg... \n","Image sizes 512 train, 512 val\n","Using 4 dataloader workers\n","Logging results to \u001b[1mFinal-Covid19-Detection/yoloV5x_300e_fold0_04/11_20:52\u001b[0m\n","Starting training for 300 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      0/299         0G     0.1097    0.02088          0         27        512:  "]}],"source":["!python train.py --img 512 \\\n","                 --batch 16 \\\n","                 --epochs {EPOCHS} \\\n","                 --data data.yaml \\\n","                 --weights yolov5x.pt \\\n","                 --save-period 50\\\n","                 --project Final-Covid19-Detection --hyp /kaggle/working/hyp.yaml --name {name}"]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.6 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":4}
